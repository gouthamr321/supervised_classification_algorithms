source("~/Development/statistical_data_mining/comp_labs/Homework_4/Hw5_p1.R", echo=TRUE)
biplot(pc_ex$x[,1], pc_ex$x[,2])
biplot(pc_ex$x[,1:2])
biplot(pc_ex)
k_values <- 1:10
error <- c()
for (i in 1:length(k_values)){
y_pred <- knn(train = data.matrix(pendigits_training[,-17]), test = data.matrix(pendigits_testing[,-17]), cl = pendigits_training[,17], k = i)
cm_knn_fit <- confusionMatrix(as.factor(y_pred),as.factor(pendigits_testing[,17]))
error[i] <- 1 - cm_knn_fit$overall[1]
}
print(paste0("the KNN with the lowest error is ", which.min(error)))
# KNN on the Principal components
pc_dataset <- data.frame(cbind(pc_ex$x[,1], pc_ex$x[,2], pc_ex$x[,3], pc_ex$x[,4], pc_ex$x[,5]))
pc_dataset_training = pc_dataset[train,]
pc_dataset_testing = pc_dataset[-train,]
error
k_values <- 1:10
error_PC <- c()
for (i in 1:length(k_values)){
y_pred <- knn(train = data.matrix(pc_dataset_training), test = data.matrix(pc_dataset_testing), cl = pendigits_training[,17], k = i)
cm_knn_fit <- confusionMatrix(as.factor(y_pred),as.factor(pendigits_testing[,17]))
error_PC[i] <- 1 - cm_knn_fit$overall[1]
}
print(paste0("the KNN for Principal components with the lowest error is ", which.min(error_PC)))
error_PC
# part b KNN--- fit KNN classifier
k_values <- 1:10
error <- c()
for (i in 1:length(k_values)){
y_pred <- knn(train = data.matrix(pendigits_training[,-17]), test = data.matrix(pendigits_testing[,-17]), cl = pendigits_training[,17], k = i)
cm_knn_fit <- confusionMatrix(as.factor(y_pred),as.factor(pendigits_testing[,17]))
error[i] <- 1 - cm_knn_fit$overall[1]
}
print(paste0("the KNN with the lowest error is ", which.min(error)))
# KNN on the Principal components
pc_dataset <- data.frame(cbind(pc_ex$x[,1], pc_ex$x[,2], pc_ex$x[,3], pc_ex$x[,4], pc_ex$x[,5]))
pc_dataset_training = pc_dataset[train,]
pc_dataset_testing = pc_dataset[-train,]
error
rf.fit <- randomForest(as.factor(class)~., data = pendigits_training, n.tree = 100)
quartz()
varImpPlot(rf.fit)
importance(rf.fit)
RF_pred_training <- predict(rf.fit, newdata = pendigits_training, type = "response")
RF_pred_testing <- predict(rf.fit, newdata = pendigits_testing, type = "response")
cm_pred_training <- confusionMatrix(as.factor(RF_pred_training), as.factor(pendigits_training$class))
cm_pred_testing <- confusionMatrix(as.factor(RF_pred_testing), as.factor(pendigits_testing$class))
# compare to fitting random forest on principal components
# append output
pc_dataset <- data.frame(cbind(pc_dataset, pendigits[,17]))
pc_dataset_training = pc_dataset[train,]
pc_dataset_testing = pc_dataset[-train,]
pc_dataset_training = pc_dataset[train,]
pc_dataset_testing = pc_dataset[-train,]
cm_pred_training
cm_pred_testing
rf.fit_PC <- randomForest(as.factor(pc_dataset_training[,6])~., data = pc_dataset_training, n.tree = 100)
quartz()
varImpPlot(rf.fit_PC)
importance(rf.fit_PC)
RF_pred_training <- predict(rf.fit_PC, newdata = pc_dataset_training, type = "response")
RF_pred_testing <- predict(rf.fit_PC, newdata = pc_dataset_testing, type = "response")
cm_pred_training <- confusionMatrix(as.factor(RF_pred_training), as.factor(pc_dataset_training[,6]))
cm_pred_testing <- confusionMatrix(as.factor(RF_pred_testing), as.factor(pc_dataset_testing[,6]))
cm_pred_testing
cm_pred_training
cm_pred_testing
?neuralnet
??neuralnet
setwd("/Users/goutham/Development/statistical_data_mining/comp_labs/hw5")
load('cleveland.Rdata')
set.seed(1)
cleveland_dat <- cleveland[-15]
cleaveland_dat_responce <- as.numeric(cleveland_dat[,14])
cleveland_dat[,14] <- cleaveland_dat_responce - 1
set.seed(10)
train = sample(1:nrow(cleveland_dat), .80*nrow(cleveland_dat))
cleveland_dat_training = cleveland_dat[train,]
cleveland_dat_testing = cleveland_dat[-train,]
# fit a neural net
cleveland_dat_nn <- cleveland_dat
cleveland_dat_nn$gender <- as.numeric(cleveland_dat$gender)
cleveland_dat_nn$cp <- as.numeric(cleveland_dat$cp)
cleveland_dat_nn$fbs <- as.numeric(cleveland_dat$fbs)
cleveland_dat_nn$restecg <- as.numeric(cleveland_dat$restecg)
cleveland_dat_nn$exang <- as.numeric(cleveland_dat$exang)
cleveland_dat_nn$slope <- as.numeric(cleveland_dat$slope)
cleveland_dat_nn$thal <- as.numeric(cleveland_dat$thal)
cleveland_dat_nn_training = cleveland_dat_nn[train,]
cleveland_dat_nn_testing = cleveland_dat_nn[-train,]
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
train_err_store <- c()
test_err_store <- c()
for (i in 1:10){
# fit neural network with "i" neurons --- i is the number of neurons in the hidden layer
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
# calculate the train error
pred <- predict(nn1, newdata = cleveland_dat_nn_training)
y_hat_train <- round(pred)
train_err <- length(which(cleveland_dat_nn_training$diag1 != y_hat_train))/length(y_hat_train)
train_err_store <- c(train_err_store, train_err) #store the error at each iteration
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
test_err <- length(which(cleveland_dat_nn_testing$diag1 != y_hat_test))/length(y_hat_test)
test_err_store <- c(test_err_store, test_err) #store the error at each iteration
}
train_err_nn
test_err_nn
print(paste0("the NN with the lowest test error is the model with hidden units= ", which.min(test_err_nn)))
library(rpart)
library(caret)
library(randomForest) # install.packages("randomForest")
library(neuralnet)
library(nnet)
setwd("/Users/goutham/Development/statistical_data_mining/comp_labs/hw5")
load('cleveland.Rdata')
set.seed(1)
cleveland_dat <- cleveland[-15]
cleaveland_dat_responce <- as.numeric(cleveland_dat[,14])
cleveland_dat[,14] <- cleaveland_dat_responce - 1
set.seed(10)
train = sample(1:nrow(cleveland_dat), .80*nrow(cleveland_dat))
cleveland_dat_training = cleveland_dat[train,]
cleveland_dat_testing = cleveland_dat[-train,]
# fit a neural net
cleveland_dat_nn <- cleveland_dat
cleveland_dat_nn$gender <- as.numeric(cleveland_dat$gender)
cleveland_dat_nn$cp <- as.numeric(cleveland_dat$cp)
cleveland_dat_nn$fbs <- as.numeric(cleveland_dat$fbs)
cleveland_dat_nn$restecg <- as.numeric(cleveland_dat$restecg)
cleveland_dat_nn$exang <- as.numeric(cleveland_dat$exang)
cleveland_dat_nn$slope <- as.numeric(cleveland_dat$slope)
cleveland_dat_nn$thal <- as.numeric(cleveland_dat$thal)
cleveland_dat_nn_training = cleveland_dat_nn[train,]
cleveland_dat_nn_testing = cleveland_dat_nn[-train,]
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
train_err_store <- c()
test_err_store <- c()
for (i in 1:10){
# fit neural network with "i" neurons --- i is the number of neurons in the hidden layer
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
# calculate the train error
pred <- predict(nn1, newdata = cleveland_dat_nn_training)
y_hat_train <- round(pred)
train_err <- length(which(cleveland_dat_nn_training$diag1 != y_hat_train))/length(y_hat_train)
train_err_store <- c(train_err_store, train_err) #store the error at each iteration
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
test_err <- length(which(cleveland_dat_nn_testing$diag1 != y_hat_test))/length(y_hat_test)
test_err_store <- c(test_err_store, test_err) #store the error at each iteration
}
train_err_nn
test_err_nn
print(paste0("the NN with the lowest test error is the model with hidden units= ", which.min(test_err_nn)))
library(rpart)
library(caret)
library(randomForest) # install.packages("randomForest")
library(neuralnet)
library(nnet)
setwd("/Users/goutham/Development/statistical_data_mining/comp_labs/hw5")
load('cleveland.Rdata')
set.seed(1)
cleveland_dat <- cleveland[-15]
cleaveland_dat_responce <- as.numeric(cleveland_dat[,14])
cleveland_dat[,14] <- cleaveland_dat_responce - 1
set.seed(10)
train = sample(1:nrow(cleveland_dat), .80*nrow(cleveland_dat))
cleveland_dat_training = cleveland_dat[train,]
cleveland_dat_testing = cleveland_dat[-train,]
# fit a neural net
cleveland_dat_nn <- cleveland_dat
cleveland_dat_nn$gender <- as.numeric(cleveland_dat$gender)
cleveland_dat_nn$cp <- as.numeric(cleveland_dat$cp)
cleveland_dat_nn$fbs <- as.numeric(cleveland_dat$fbs)
cleveland_dat_nn$restecg <- as.numeric(cleveland_dat$restecg)
cleveland_dat_nn$exang <- as.numeric(cleveland_dat$exang)
cleveland_dat_nn$slope <- as.numeric(cleveland_dat$slope)
cleveland_dat_nn$thal <- as.numeric(cleveland_dat$thal)
cleveland_dat_nn_training = cleveland_dat_nn[train,]
cleveland_dat_nn_testing = cleveland_dat_nn[-train,]
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
train_err_store <- c()
test_err_store <- c()
for (i in 1:10){
# fit neural network with "i" neurons --- i is the number of neurons in the hidden layer
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
# calculate the train error
pred <- predict(nn1, newdata = cleveland_dat_nn_training)
y_hat_train <- round(pred)
train_err <- length(which(cleveland_dat_nn_training$diag1 != y_hat_train))/length(y_hat_train)
train_err_store <- c(train_err_store, train_err) #store the error at each iteration
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
test_err <- length(which(cleveland_dat_nn_testing$diag1 != y_hat_test))/length(y_hat_test)
test_err_store <- c(test_err_store, test_err) #store the error at each iteration
}
train_err_nn
test_err_nn
print(paste0("the NN with the lowest test error is the model with hidden units= ", which.min(test_err_nn)))
library(rpart)
library(caret)
library(randomForest) # install.packages("randomForest")
library(neuralnet)
library(nnet)
setwd("/Users/goutham/Development/statistical_data_mining/comp_labs/hw5")
load('cleveland.Rdata')
set.seed(1)
cleveland_dat <- cleveland[-15]
cleaveland_dat_responce <- as.numeric(cleveland_dat[,14])
cleveland_dat[,14] <- cleaveland_dat_responce - 1
set.seed(10)
train = sample(1:nrow(cleveland_dat), .80*nrow(cleveland_dat))
cleveland_dat_training = cleveland_dat[train,]
cleveland_dat_testing = cleveland_dat[-train,]
# fit a neural net
cleveland_dat_nn <- cleveland_dat
cleveland_dat_nn$gender <- as.numeric(cleveland_dat$gender)
cleveland_dat_nn$cp <- as.numeric(cleveland_dat$cp)
cleveland_dat_nn$fbs <- as.numeric(cleveland_dat$fbs)
cleveland_dat_nn$restecg <- as.numeric(cleveland_dat$restecg)
cleveland_dat_nn$exang <- as.numeric(cleveland_dat$exang)
cleveland_dat_nn$slope <- as.numeric(cleveland_dat$slope)
cleveland_dat_nn$thal <- as.numeric(cleveland_dat$thal)
cleveland_dat_nn_training = cleveland_dat_nn[train,]
cleveland_dat_nn_testing = cleveland_dat_nn[-train,]
#nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
train_err_store <- c()
test_err_store <- c()
for (i in 1:10){
# fit neural network with "i" neurons --- i is the number of neurons in the hidden layer
nn1 <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = i, stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
# calculate the train error
pred <- predict(nn1, newdata = cleveland_dat_nn_training)
y_hat_train <- round(pred)
train_err <- length(which(cleveland_dat_nn_training$diag1 != y_hat_train))/length(y_hat_train)
train_err_store <- c(train_err_store, train_err) #store the error at each iteration
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
test_err <- length(which(cleveland_dat_nn_testing$diag1 != y_hat_test))/length(y_hat_test)
test_err_store <- c(test_err_store, test_err) #store the error at each iteration
}
train_err_nn
train_err_store
test_err_store
print(paste0("the NN with the lowest test error is the model with hidden units= ", which.min(test_err_nn)))
train_err_store
test_err_store
print(paste0("the NN with the lowest test error is the model with hidden units= ", which.min(test_err_store)))
# fit a CART model
model.controls <- rpart.control(minbucket = 5, minsplit = 40, xval = 10, cp = 0)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing
model.controls <- rpart.control(minbucket = 5, minsplit = 40, xval = 10, cp = 0.1)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
model.controls <- rpart.control(minbucket = 5, minsplit = 10, xval = 10, cp = 0.1)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
model.controls <- rpart.control(minbucket = 5, minsplit = 5, xval = 10, cp = 0.5)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
fit_cleveland
fit_cleveland$cptable
model.controls <- rpart.control(minbucket = 5, minsplit = 5, xval = 10, cp = 0:5)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
min_cp
fit_cleveland$cptable
model.controls <- rpart.control(minbucket = 5, minsplit = 5, xval = 10, cp = 0:5)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
View(cleveland)
model.controls <- rpart.control(minbucket = 5, minsplit = 5, xval = 10, cp = 0)
fit_cleveland <- rpart(diag1~., data = cleveland_dat_training, control = model.controls)
min_cp = which.min(fit_cleveland$cptable[,4])
quartz()
# plot to find the best complexity parameter
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
min_cp
fit_cleveland$cptable
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
plot(rev(fit_cleveland$cptable[,1]), rev(fit_cleveland$cptable[,4]), col = "blue", type = "b", xlab = "complexity", ylab = "cross validation error", ylim = c(0,1), main="CART error vs. complexity parameter")
pruned_fit_cleveland <- prune(fit_cleveland, cp = fit_cleveland$cptable[min_cp, 1])
plot(pruned_fit_cleveland, branch = .3, compress=T, main = "Pruned Tree")
text(pruned_fit_cleveland, cex = .5)
CART_pred_training <- predict(pruned_fit_cleveland, newdata = cleveland_dat_training)
CART_pred_training <- (CART_pred_training > 0.5) * 1
cm_pred_training <- confusionMatrix(as.factor(CART_pred_training),as.factor(cleveland_dat_training$diag1))
CART_pred_testing <- predict(pruned_fit_cleveland, newdata = cleveland_dat_testing)
CART_pred_testing <- (CART_pred_testing > 0.5) * 1
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing
cm_pred_training
1-0.7669
cm_pred_testing
1-0.7667
rf.fit <- randomForest(as.factor(diag1)~., data = cleveland_dat_training, n.tree = 10000)
quartz()
varImpPlot(rf.fit)
importance(rf.fit)
RF_pred_training <- predict(rf.fit, newdata = cleveland_dat_training, type = "response")
RF_pred_testing <- predict(rf.fit, newdata = cleveland_dat_testing, type = "response")
cm_pred_training <- confusionMatrix(as.factor(RF_pred_training),as.factor(cleveland_dat_training$diag1))
cm_pred_testing <- confusionMatrix(as.factor(RF_pred_testing),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing
cm_pred_training
1- 0.8667
which.min(test_err_store)
# fitting the best neural network again and getting a confusion matrix
nn_opt < neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = which.min(test_err_store), stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
cm_pred_testing_nn <- confusionMatrix(as.factor(y_hat_test),as.factor(cleveland_dat_testing$diag1))
nn_opt <- neuralnet(diag1 ~ ., data = cleveland_dat_nn_training, hidden = which.min(test_err_store), stepmax = 10^9, err.fct = "ce", linear.output = FALSE)
pred <- predict(nn1, newdata = cleveland_dat_nn_testing)
y_hat_test <- round(pred)
cm_pred_testing_nn <- confusionMatrix(as.factor(y_hat_test),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing_nn
cm_pred_testing <- confusionMatrix(as.factor(CART_pred_testing),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing
clear
clc
cm_pred_testing
rf.fit <- randomForest(as.factor(diag1)~., data = cleveland_dat_training, n.tree = 10000)
quartz()
varImpPlot(rf.fit)
importance(rf.fit)
RF_pred_training <- predict(rf.fit, newdata = cleveland_dat_training, type = "response")
RF_pred_testing <- predict(rf.fit, newdata = cleveland_dat_testing, type = "response")
cm_pred_training <- confusionMatrix(as.factor(RF_pred_training),as.factor(cleveland_dat_training$diag1))
cm_pred_testing <- confusionMatrix(as.factor(RF_pred_testing),as.factor(cleveland_dat_testing$diag1))
cm_pred_testing
ls
cm_pred_testing
cm_pred_testing
#3) This problem involves the OJ data set which is part of the ISLR2 package.
rm(list = ls())
library(ISLR2)
library(caret)
library(e1071)
library(MASS)
data(OJ)
dataset_OJ <- OJ
#(a) Create a training set containing a random sample of 800 observations, and a
#test set containing the remaining observations.
set.seed(1)
percent_training <- 800/nrow(dataset_OJ)
dataset_OJ$Purchase <- as.numeric(dataset_OJ[,1]) - 1
train <- sample(1:nrow(dataset_OJ), percent_training* nrow(dataset_OJ))
dataset_OJ_training = dataset_OJ[train,]
dataset_OJ_testing = dataset_OJ[-train,]
dataset_OJ
fit <- svm(Purchase~., data = dataset_OJ_training, kernel = "linear", cost = 0.01, scale = FALSE)
summary(fit)
fit <- svm(Purchase~., data = dataset_OJ_training, kernel = "linear", cost = 0.01, scale = FALSE)
y_hat_training <- (predict(fit, newdata = dataset_OJ_training) > 0.5) * 1
y_true_training <- dataset_OJ_training$Purchase
cm_pred_training <- confusionMatrix(as.factor(y_hat_training),as.factor(y_true_training))
summary(fit)
y_hat_testing <- (predict(fit, newdata = dataset_OJ_testing) > 0.5) * 1
y_true_testing <- dataset_OJ_testing$Purchase
cm_pred_testing <- confusionMatrix(as.factor(y_hat_testing),as.factor(y_true_testing))
m_pred_testing <- confusionMatrix(as.factor(y_hat_testing),as.factor(y_true_testing))
cm_pred_testing <- confusionMatrix(as.factor(y_hat_testing),as.factor(y_true_testing))
cm_pred_testing
cm_pred_training
1-0.8175
1 - 0.8037
tune.model.rad <- tune(svm, Purchase ~., data = dataset_OJ_training, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 3, 5, 7, 10)))
summary(tune.model.rad)
names(tune.model.rad)
bestmod <- tune.model.rad$best.model
bestmod
summary(tune.model.rad)
y_hat_train_optimized <- (predict(bestmod, newdata = dataset_OJ_training) > 0.5) * 1
y_hat_test_optimized <- (predict(bestmod, newdata = dataset_OJ_testing) > 0.5) * 1
cm_pred_training_opt <- confusionMatrix(as.factor(y_hat_train_optimized), as.factor(y_true_training))
cm_pred_testing_opt <- confusionMatrix(as.factor(y_hat_test_optimized),as.factor(y_true_testing))
cm_pred_training_opt
1- 0.8238
cm_pred_testing_opt <- confusionMatrix(as.factor(y_hat_test_optimized),as.factor(y_true_testing))
cm_pred_testing_opt
1 - 0.8333
tune.model.rad_kernal <- tune(svm, Purchase ~., data = dataset_OJ_training, kernel = "radial", ranges = list(cost = c(.001, .01, .1, 1, 3, 5, 7, 10)))
summary(tune.model.rad_kernal)
names(tune.model.rad_kernal)
bestmod <- tune.model.rad_kernal$best.model
y_hat_train_optimized_kernal <- (predict(bestmod, newdata = dataset_OJ_training) > 0.5) * 1
y_hat_test_optimized_kernal <- (predict(bestmod, newdata = dataset_OJ_testing) > 0.5) * 1
cm_pred_training_opt_kernal <- confusionMatrix(as.factor(y_hat_train_optimized_kernal), as.factor(y_true_training))
cm_pred_testing_opt_kenral <- confusionMatrix(as.factor(y_hat_test_optimized_kernal),as.factor(y_true_testing))
summary(tune.model.rad_kernal)
y_hat_train_optimized_kernal <- (predict(bestmod, newdata = dataset_OJ_training) > 0.5) * 1
y_hat_train_optimized_kernal
y_hat_test_optimized_kernal
clear
clc
cm_pred_training_opt_kernal
1- 0.8475
cm_pred_testing_opt_kenral <- confusionMatrix(as.factor(y_hat_test_optimized_kernal),as.factor(y_true_testing))
cm_pred_testing_opt_kenral
1 - 0.8148
tune.model.rad_kernal <- tune(svm, Purchase ~., data = dataset_OJ_training, kernel = "polynomial", degree = 2, ranges = list(cost = c(.001, .01, .1, 1, 3, 5, 7, 10)))
summary(tune.model.rad_kernal)
names(tune.model.rad_kernal)
bestmod <- tune.model.rad_kernal$best.model
tune.model.poly_kernal <- tune(svm, Purchase ~., data = dataset_OJ_training, kernel = "polynomial", degree = 2, ranges = list(cost = c(.001, .01, .1, 1, 3, 5, 7, 10)))
summary(tune.model.poly_kernal)
names(tune.model.poly_kernal)
bestmod <- tune.model.poly_kernal$best.model
summary(tune.model.poly_kernal)
y_hat_train_optimized_kernal <- (predict(bestmod, newdata = dataset_OJ_training) > 0.5) * 1
y_hat_test_optimized_kernal <- (predict(bestmod, newdata = dataset_OJ_testing) > 0.5) * 1
cm_pred_training_opt_kernal <- confusionMatrix(as.factor(y_hat_train_optimized_kernal), as.factor(y_true_training))
cm_pred_testing_opt_kenral <- confusionMatrix(as.factor(y_hat_test_optimized_kernal),as.factor(y_true_testing))
cm_pred_training_opt_kernal
cm_pred_testing_opt_kenral <- confusionMatrix(as.factor(y_hat_test_optimized_kernal),as.factor(y_true_testing))
cm_pred_testing_opt_kenral
1 - 0.7704
1 - 0.8262
source("~/Development/statistical_data_mining/comp_labs/hw5/hw5_p2.R", echo=TRUE)
source("~/Development/statistical_data_mining/comp_labs/Homework_4/Hw5_p1.R", echo=TRUE)
source("~/Development/statistical_data_mining/comp_labs/hw5/hw5_p3.R", echo=TRUE)
